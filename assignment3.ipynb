{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper functions ---\n",
    "def batch_iter(n_samples, batch_size, rng):\n",
    "    while True:\n",
    "        idx = np.arange(n_samples)\n",
    "        rng.shuffle(idx)\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            yield idx[start:start + batch_size]\n",
    "\n",
    "class MiniBatchLogReg:\n",
    "    def __init__(self, dim, rng=None):\n",
    "        self.rng = np.random.default_rng() if rng is None else rng\n",
    "        self.w = self.rng.normal(scale=0.01, size=dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def loss_grad(self, X, y):\n",
    "        z = X @ self.w\n",
    "        preds = self.sigmoid(z)\n",
    "        eps = 1e-12\n",
    "        loss = -np.mean(y*np.log(preds+eps) + (1-y)*np.log(1-preds+eps))\n",
    "        grad = X.T @ (preds - y) / len(y)\n",
    "        return loss, grad\n",
    "\n",
    "    def fit(self, X, y, batch_size=32, lr=0.01, max_iters=5000, verbose=False):\n",
    "        n = len(y)\n",
    "        rng = self.rng\n",
    "        biter = batch_iter(n, batch_size, rng)\n",
    "        for it in range(max_iters):\n",
    "            idx = next(biter)\n",
    "            loss, grad = self.loss_grad(X[idx], y[idx])\n",
    "            self.w -= lr * grad\n",
    "            if verbose and it % 1000 == 0:\n",
    "                print(f\"iter {it} loss {loss:.4f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X @ self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      "Train: [127 214]\n",
      "Val: [42 72]\n",
      "Test: [43 71]\n"
     ]
    }
   ],
   "source": [
    "# (a) load and preprocess dataset ---\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# (b) train-val-test split\n",
    "# stratified train (60%) temp (40%)\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "\n",
    "# split temp into val and test (each 20% total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=42)\n",
    "\n",
    "# standardize features based on training set\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# (c) - class counts\n",
    "train_counts = np.bincount(y_train)\n",
    "val_counts = np.bincount(y_val)\n",
    "test_counts = np.bincount(y_test)\n",
    "\n",
    "print(\"Class counts:\")\n",
    "print(\"Train:\", train_counts)\n",
    "print(\"Val:\", val_counts)\n",
    "print(\"Test:\", test_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>split</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>16</td>\n",
       "      <td>train</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.981567</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.988399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>16</td>\n",
       "      <td>test</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>64</td>\n",
       "      <td>train</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.981567</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.988399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100</td>\n",
       "      <td>64</td>\n",
       "      <td>test</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100</td>\n",
       "      <td>128</td>\n",
       "      <td>train</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.981567</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.988399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.100</td>\n",
       "      <td>128</td>\n",
       "      <td>test</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>train</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.981567</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.988399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>test</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.978723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>train</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.981567</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.988399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>test</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.978723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>train</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.981567</td>\n",
       "      <td>0.995327</td>\n",
       "      <td>0.988399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>test</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.978723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>train</td>\n",
       "      <td>0.970674</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.971963</td>\n",
       "      <td>0.976526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>test</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.964539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>train</td>\n",
       "      <td>0.970674</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.971963</td>\n",
       "      <td>0.976526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>test</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.964539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>train</td>\n",
       "      <td>0.973607</td>\n",
       "      <td>0.981221</td>\n",
       "      <td>0.976636</td>\n",
       "      <td>0.978923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>test</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.964539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr  batch_size  split  accuracy  precision    recall        f1\n",
       "0   0.100          16  train  0.985337   0.981567  0.995327  0.988399\n",
       "1   0.100          16   test  0.982456   0.985915  0.985915  0.985915\n",
       "2   0.100          64  train  0.985337   0.981567  0.995327  0.988399\n",
       "3   0.100          64   test  0.982456   0.985915  0.985915  0.985915\n",
       "4   0.100         128  train  0.985337   0.981567  0.995327  0.988399\n",
       "5   0.100         128   test  0.982456   0.985915  0.985915  0.985915\n",
       "6   0.010          16  train  0.985337   0.981567  0.995327  0.988399\n",
       "7   0.010          16   test  0.973684   0.985714  0.971831  0.978723\n",
       "8   0.010          64  train  0.985337   0.981567  0.995327  0.988399\n",
       "9   0.010          64   test  0.973684   0.985714  0.971831  0.978723\n",
       "10  0.010         128  train  0.985337   0.981567  0.995327  0.988399\n",
       "11  0.010         128   test  0.973684   0.985714  0.971831  0.978723\n",
       "12  0.001          16  train  0.970674   0.981132  0.971963  0.976526\n",
       "13  0.001          16   test  0.956140   0.971429  0.957746  0.964539\n",
       "14  0.001          64  train  0.970674   0.981132  0.971963  0.976526\n",
       "15  0.001          64   test  0.956140   0.971429  0.957746  0.964539\n",
       "16  0.001         128  train  0.973607   0.981221  0.976636  0.978923\n",
       "17  0.001         128   test  0.956140   0.971429  0.957746  0.964539"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hyperparameter grid\n",
    "learning_rates = [1e-1, 1e-2, 1e-3]\n",
    "batch_sizes = [16, 64, 128]\n",
    "\n",
    "results = []\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        model = MiniBatchLogReg(dim=X_train.shape[1], rng=np.random.default_rng(0))\n",
    "        model.fit(X_train, y_train, batch_size=bs, lr=lr, max_iters=5000, verbose=False)\n",
    "\n",
    "        # evaluate on train and test\n",
    "        for split_name, X_split, y_split in [\n",
    "            (\"train\", X_train, y_train),\n",
    "            (\"test\", X_test, y_test),\n",
    "        ]:\n",
    "            y_pred = model.predict(X_split)\n",
    "            acc = accuracy_score(y_split, y_pred)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_split, y_pred, average='binary', zero_division=0)\n",
    "            results.append({\n",
    "                \"lr\": lr,\n",
    "                \"batch_size\": bs,\n",
    "                \"split\": split_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "- The largest learning rate 1e-1 generalizes the test set the best. \n",
    "- The batch size does not effect the final result a lot. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
